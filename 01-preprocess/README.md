### Preprocess Folder

This folder contains very simple scripts to preprocess the raw BabyLM 2024 data.
First, place the raw data in `../data/eng/` by distributing them into the appropriate sub-directory (each sub-directory can contain one or more simple-text, UTF-8, files):
`childes`, `subtitles`, `conversations` (this folder can include bnc_spoken corpus), `gutenberg` and `wikipedia`.
Then, each script can be run individually. 

For the full batch on SLURM, run the following command (you need to create your CONDA environment optimized for your GPUs):

```bash
./clean.sbatch
```

Required python packages (to be installed before running the script):

 - `smart-open`
 - `normalize`
 - `ftfy`


After running `clean.sbatch` you will have one single processed file for each dataset, as well as unique file:

 - `all.txt`: All the datasets concatenated to create your tokenizer and train your model.
 - `typology.txt`: The files generated by processing each folder.


The preprocessed data files will be stored in `../data/eng/processed/`.